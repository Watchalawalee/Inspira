import os
import json
import pandas as pd
from glob import glob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from collections import defaultdict
import dateparser
import sys
from datetime import datetime
import re
import requests
import time


# ------------------------
# CONFIG
# ------------------------
base_path = os.path.abspath("./scrapy_project/spiders")
eps = 0.3  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡∏≠‡∏á clustering

# ------------------------
# Helper Functions
# ------------------------
def is_empty(value):
    return value in [None, "", "null", "NULL", "-", [], {}] or (
        isinstance(value, str) and value.strip().lower() == "null"
    )

def normalize_field(key, value):
    if key == "ticket_price":
        if isinstance(value, str) and value.isdigit():
            return [int(value)]
        if isinstance(value, int):
            return [value]
    return value

def get_year_from_date(date_str):
    dt = dateparser.parse(date_str, languages=['th', 'en'])
    return dt.year if dt else None

thai_month_map = {
    "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": "01", "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": "02", "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": "03", "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": "04",
    "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": "05", "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": "06", "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": "07", "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": "08",
    "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": "09", "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": "10", "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": "11", "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": "12"
}

def parse_thai_date_to_obj(date_str):
    try:
        day, month_thai, year = date_str.strip().split(" ")
        month = thai_month_map.get(month_thai)
        year = int(year)
        if year > 2400:
            year -= 543  # ‡πÅ‡∏õ‡∏•‡∏á ‡∏û.‡∏®. ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®.
        return datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
    except Exception:
        return None

# ------------------------
# Merge Logic
# ------------------------
def smart_merge_fully_matched(a, b):
    merged = {}
    skip_ticket_price = False
    reliability_a = a.get("reliability_score", 0)
    reliability_b = b.get("reliability_score", 0)
    timestamp_a = a.get("timestamp", "")
    timestamp_b = b.get("timestamp", "")

    for key in set(a.keys()).union(b.keys()):
        a_val = normalize_field(key, a.get(key))
        b_val = normalize_field(key, b.get(key))

        if key == "ticket_price" and skip_ticket_price:
            continue

        if key == "ticket":
            if str(a_val).strip() == "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°":
                merged["ticket"] = a_val
                merged["ticket_price"] = normalize_field("ticket_price", a.get("ticket_price"))
                skip_ticket_price = True
                continue
            elif str(b_val).strip() == "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°":
                merged["ticket"] = b_val
                merged["ticket_price"] = normalize_field("ticket_price", b.get("ticket_price"))
                skip_ticket_price = True
                continue
            else:
                merged["ticket"] = a_val if reliability_a >= reliability_b else b_val
                continue

        if key == "title":
            if reliability_a > reliability_b:
                merged[key] = a_val
            elif reliability_b > reliability_a:
                merged[key] = b_val
            else:
                merged[key] = a_val if timestamp_a > timestamp_b else b_val
            continue

        if is_empty(a_val) and not is_empty(b_val):
            merged[key] = b_val
        elif not is_empty(a_val) and is_empty(b_val):
            merged[key] = a_val
        elif key == "status":
            if a_val != b_val:
                if reliability_a > reliability_b:
                    merged[key] = a_val
                elif reliability_b > reliability_a:
                    merged[key] = b_val
                else:
                    merged[key] = a_val if timestamp_a > timestamp_b else b_val
            else:
                merged[key] = a_val
    
        elif not is_empty(a_val) and not is_empty(b_val):
            if key == "description":
                merged[key] = a_val if len(str(a_val)) >= len(str(b_val)) else b_val
                merged["categories"] = a.get("categories") if len(str(a_val)) >= len(str(b_val)) else b.get("categories")
            elif key in ["url", "cover_picture"]:
                if reliability_a > reliability_b:
                    merged[key] = a_val
                elif reliability_b > reliability_a:
                    merged[key] = b_val
                else:
                    merged[key] = a_val if timestamp_a > timestamp_b else b_val
            elif key not in ["categories"]:
                merged[key] = a_val if len(str(a_val)) >= len(str(b_val)) else b_val
        else:
            merged[key] = None

    return merged

def compare_records(a, b):
    merged = {}
    reliability_a = a.get("reliability_score", 0)
    reliability_b = b.get("reliability_score", 0)
    timestamp_a = a.get("timestamp", "")
    timestamp_b = b.get("timestamp", "")

    core_fields = ["start_date", "end_date", "location"]
    all_core_equal = all(a.get(f, "") == b.get(f, "") for f in core_fields)

    if all_core_equal:
        return smart_merge_fully_matched(a, b)

    for field in core_fields:
        a_val = a.get(field)
        b_val = b.get(field)
        if a_val == b_val:
            merged[field] = a_val
        else:
            if reliability_a > reliability_b:
                merged[field] = a_val
            elif reliability_b > reliability_a:
                merged[field] = b_val
            else:
                merged[field] = a_val if timestamp_a > timestamp_b else b_val

    other_merged = smart_merge_fully_matched(a, b)
    for key, val in other_merged.items():
        if key not in merged:
            merged[key] = val

    return merged

# ------------------------
# Data Loading
# ------------------------
def load_all_json(base_path, mode="full"):
    all_data = []
    mode = mode.lower()
    assert mode in ["full", "upcoming"], "mode ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 'full' ‡∏´‡∏£‡∏∑‡∏≠ 'upcoming' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    for root, dirs, files in os.walk(base_path):
        target_dir = os.path.join(root, "raw_data", mode)
        print(f"üîç Looking in: {target_dir}")

        if os.path.exists(target_dir):
            json_files = glob(os.path.join(target_dir, "*.json"))
            print(f"üì¶ Found {len(json_files)} JSON files")

            for file in json_files:
                print(f"üìÑ Reading: {file}")
                try:
                    with open(file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            for d in data:
                                d["source_file"] = file
                                all_data.append(d)
                        else:
                            data["source_file"] = file
                            all_data.append(data)
                except Exception as e:
                    print(f"‚ùå Error reading {file}: {e}")
        else:
            print(f"üö´ Not found: {target_dir}")
    return all_data

# ------------------------
# üîç ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ start_date ‡∏´‡∏£‡∏∑‡∏≠ end_date
# ------------------------
def is_invalid_date(value):
    return value in [None, "", "null", "NULL"]

def filter_valid_events(events):
    return [
        e for e in events
        if not is_invalid_date(e.get("start_date")) and not is_invalid_date(e.get("end_date"))
    ]

def is_invalid_title_or_source(event):
    title = str(event.get("title", "")).strip().lower()
    source_file = str(event.get("source_file", "")).strip().lower()

    invalid_values = {"", "none", "null", "-", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "n/a", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"}

    return title in invalid_values or source_file in invalid_values

def filter_clean_events(events):
    """ ‡∏Å‡∏£‡∏≠‡∏á event ‡∏ó‡∏µ‡πà title ‡πÅ‡∏•‡∏∞ source_file ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ """
    return [e for e in events if not is_invalid_title_or_source(e)]

# ------------------------
# Clustering + Merge
# ------------------------
def merge_similar_events(all_events, eps=eps, min_samples=1):
    if not all_events:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ merge")
        return []

    titles = [event.get("title", "") for event in all_events if event.get("title")]
    if not titles:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ title ‡πÉ‡∏´‡πâ clustering")
        return []

    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(titles)

    model = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
    labels = model.fit_predict(X)

    clusters = defaultdict(list)
    for idx, label in enumerate(labels):
        clusters[label].append(all_events[idx])

    merged_events = []
    for label, group in clusters.items():
        merged = group[0]
        for other in group[1:]:
            merged = compare_records(merged, other)
        merged_events.append(merged)

    return merged_events

# ------------------------
# Run the pipeline
# ------------------------
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏∞‡∏ö‡∏∏ mode ‡∏î‡πâ‡∏ß‡∏¢ --mode=full ‡∏´‡∏£‡∏∑‡∏≠ --mode=upcoming")
        sys.exit(1)

    mode_arg = sys.argv[1]
    if mode_arg.startswith("--mode="):
        mode = mode_arg.split("=")[1].lower()
    else:
        print("‚ùå ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ --mode=full ‡∏´‡∏£‡∏∑‡∏≠ --mode=upcoming")
        sys.exit(1)

    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Merge Mode: {mode}")

    all_json_data = load_all_json(base_path, mode=mode)
    print(f"üì¶ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_json_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å '{mode}'")

    valid_json_data = filter_valid_events(all_json_data)
    cleaned_json_data = filter_clean_events(valid_json_data)
    print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á title/source_file: {len(cleaned_json_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    merged_data = merge_similar_events(cleaned_json_data)
    print(f"‚úÖ ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á merge ‡πÅ‡∏•‡πâ‡∏ß: {len(merged_data)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
    # ‚úÖ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏ã‡∏ü
    df = pd.DataFrame(merged_data)

    # 1. ‡πÅ‡∏Å‡πâ categories ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á ‡πÄ‡∏õ‡πá‡∏ô ["Others"]
    def fix_categories(x):
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return ["Others"]
        elif isinstance(x, list):
            return x if len(x) > 0 else ["Others"]
        else:
            return ["Others"]

    df['categories'] = df['categories'].apply(fix_categories)

    # 2. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà start_date, end_date, location ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠ 'null', 'none', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡∏Ø‡∏•‡∏Ø
    null_values = {"", "null", "none", "-", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "n/a"}
    for col in ['start_date', 'end_date', 'location']:
        df = df[~df[col].astype(str).str.strip().str.lower().isin(null_values)]

    # 3. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà status == 'unknown'
    df = df[df['status'] != 'unknown']

    # 4. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà start_date ‡∏´‡∏£‡∏∑‡∏≠ end_date ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô "22 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° 2025")
    thai_months = {
        "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°", "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå", "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°", "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô", "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°", "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô",
        "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°", "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°", "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô", "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°", "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô", "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°"
    }

    def is_valid_thai_date(date_str):
        if not isinstance(date_str, str):
            return False
        date_str = date_str.replace('\u3000', ' ').strip()  # ‡πÅ‡∏Å‡πâ full-width space

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡∏û.‡∏®. ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ dateparser
        year_match = re.search(r"\b(25\d{2})\b", date_str)
        if year_match:
            buddhist_year = int(year_match.group(1))
            if 2500 <= buddhist_year <= 2600:
                gregorian_year = buddhist_year - 543
                date_str = re.sub(str(buddhist_year), str(gregorian_year), date_str)

        parsed = dateparser.parse(date_str, languages=['th', 'en'])
        if not parsed:
            return False
        return 2020 <= parsed.year <= 2100

    df = df[df['start_date'].apply(is_valid_thai_date)]
    df = df[df['end_date'].apply(is_valid_thai_date)]
    df['ticket_price'] = df['ticket_price'].apply(lambda x: x if isinstance(x, list) else None)
    
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M")
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå merge_data ‡∏Ç‡πâ‡∏≤‡∏á‡πÜ merge.py
    output_dir = os.path.join(os.path.dirname(__file__), "merge_data")
    os.makedirs(output_dir, exist_ok=True)

    # üîÅ ‡πÇ‡∏´‡∏•‡∏î cache ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    geocode_cache_path = os.path.join(output_dir, "geocode_cache.json")
    if os.path.exists(geocode_cache_path):
        with open(geocode_cache_path, "r", encoding="utf-8") as f:
            geocode_cache = json.load(f)
    else:
        geocode_cache = {}

    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å location
    def normalize_location(location):
        if not isinstance(location, str):
            return location

        location = location.strip()

        # üîÅ ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "impact" ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô "‡∏≠‡∏¥‡∏°‡πÅ‡∏û‡πá‡∏Ñ ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏≠‡∏á‡∏ò‡∏≤‡∏ô‡∏µ"
        if "impact" in location.lower():
            return "‡∏≠‡∏¥‡∏°‡πÅ‡∏û‡πá‡∏Ñ ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏≠‡∏á‡∏ò‡∏≤‡∏ô‡∏µ"

        # üîÅ ‡∏Å‡∏£‡∏ì‡∏µ Paragon Hall ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        if "paragon hall" in location.lower():
            return "Paragon Hall"

        # üîÅ ‡πÅ‡∏°‡∏õ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å
        known_replacements = {
            "Plenary Hall QSNCC": "‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏™‡∏¥‡∏£‡∏¥‡∏Å‡∏¥‡∏ï‡∏¥‡πå",
            "QSNCC": "‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏™‡∏¥‡∏£‡∏¥‡∏Å‡∏¥‡∏ï‡∏¥‡πå",
            "BITEC": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "EH106 ‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "EH106": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ‡∏Æ‡∏≠‡∏•‡∏•‡πå": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "BHIRAJ HALL BITEC": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "BHIRAJ HALL": "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ ‡∏ö‡∏≤‡∏á‡∏ô‡∏≤",
            "Creative Space ‡∏ä‡∏±‡πâ‡∏ô L1 ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ D": "TCDC",
            "Creative Space ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ D": "TCDC",
            "TCDC ‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ D": "TCDC",
            "‡∏´‡∏≠‡∏®‡∏¥‡∏•‡∏õ‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£": "BACC",
            "BACC": "BACC",
            "‡∏£‡∏¥‡πÄ‡∏ß‡∏≠‡∏£‡πå ‡∏ã‡∏¥‡∏ï‡∏µ‡πâ ‡πÅ‡∏ö‡∏á‡∏Ñ‡πá‡∏≠‡∏Å": "River City Bangkok",
            "‡∏™‡∏≤‡∏°‡∏¢‡πà‡∏≤‡∏ô ‡∏°‡∏¥‡∏ï‡∏£‡∏ó‡∏≤‡∏ß‡∏ô‡πå": "Samyan Mitrtown",
            "‡∏ã‡∏µ‡∏Ñ‡∏≠‡∏ô‡∏™‡πÅ‡∏Ñ‡∏ß‡∏£‡πå": "Seacon Square",
            "‡πÅ‡∏ü‡∏ä‡∏±‡πà‡∏ô‡πÑ‡∏≠‡∏™‡πå‡πÅ‡∏•‡∏ô‡∏î‡πå": "Fashion Island",
            "‡∏û‡∏≤‡∏£‡∏≤‡∏Å‡∏≠‡∏ô ‡∏Æ‡∏≠‡∏•‡∏•‡πå": "Paragon Hall",
            "‡∏ó‡∏£‡∏π ‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô ‡∏Æ‡∏≠‡∏•‡∏•‡πå": "TRUE ICON HALL",
            "‡∏≠‡∏±‡∏ï‡∏ï‡∏≤ ‡πÅ‡∏Å‡∏•‡πÄ‡∏•‡∏≠‡∏£‡∏µ‡πà": "Atta Gallery",
            "‡πÅ‡∏°‡∏î - ‡∏°‡∏±‡∏ô ‡∏°‡∏±‡∏ô ‡∏≠‡∏≤‡∏£‡πå‡∏ï ‡πÄ‡∏î‡∏™‡∏ó‡∏¥‡πÄ‡∏ô‡∏ä‡∏±‡πà‡∏ô": "MAD Art Destination",
            "Xspace": "Xspace Bangkok",
            "Explode Gallery": "Explode Gallery Bangkok",
            "Agni Gallery": "Agni Gallery Bangkok",
            "La Lanta Fine Art": "La Lanta Fine Art Bangkok",
            "HOP - Hub of Photography": "HOP Photo Gallery Bangkok"
        }

        if " - " in location:
            location = location.split(" - ")[0].strip()

        for key, val in known_replacements.items():
            if key in location:
                return val

        # üßΩ ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢ regex
        remove_patterns = [
            r"Hall\s?\d+[-\d]*",              
            r"‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ‡∏Æ‡∏≠‡∏•‡∏•‡πå\s?\d+[-\d]*",        
            r"EH\d+",                         
            r"Plenary Hall",                 
            r"‡∏ä‡∏±‡πâ‡∏ô\s?[A-Za-z0-9]+",           
            r"‡∏´‡πâ‡∏≠‡∏á\s?\d+[-\d]*",              
            r"\b‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°\b",               
            r"\b‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà\b",                  
            r"\bZone\s?[A-Za-z]+\b",         
            r"‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£\s?[A-Za-z0-9]+",         
            r"The Portal Lifestyle Complex", 
        ]

        for pattern in remove_patterns:
            location = re.sub(pattern, "", location, flags=re.IGNORECASE)

        location = re.sub(r"\s{2,}", " ", location).strip()

        return location

    GOOGLE_API_KEY = "AIzaSyARUJc-U7xfVvWsV4LnguUoIZQcvoRM2ik"

    def get_lat_lon_from_location_google(location, geocode_cache):
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡πÉ‡∏ô cache ‡πÅ‡∏•‡πâ‡∏ß
        if location in geocode_cache:
            return geocode_cache[location]

        url = "https://maps.googleapis.com/maps/api/geocode/json"
        params = {
            "address": location,
            "key": GOOGLE_API_KEY
        }

        try:
            response = requests.get(url, params=params)
            data = response.json()

            if data["status"] == "OK":
                result = data["results"][0]["geometry"]["location"]
                lat, lon = result["lat"], result["lng"]
                geocode_cache[location] = {"lat": lat, "lon": lon}
                time.sleep(0.3)  # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                return {"lat": lat, "lon": lon}
            else:
                print(f"‚ùå [Google] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{location}' | Status: {data['status']}")
        except Exception as e:
            print(f"‚ùå [Google] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö '{location}': {e}")

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô None
        geocode_cache[location] = None
        return None
    
    def get_lat_lon_from_places_api(location, geocode_cache):
        if location in geocode_cache:
            return geocode_cache[location]

        url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
        params = {
            "query": location,
            "key": GOOGLE_API_KEY
        }

        try:
            response = requests.get(url, params=params)
            data = response.json()

            if data["status"] == "OK" and len(data["results"]) > 0:
                place = data["results"][0]
                lat = place["geometry"]["location"]["lat"]
                lon = place["geometry"]["location"]["lng"]
                geocode_cache[location] = {"lat": lat, "lon": lon}
                time.sleep(0.3)
                return {"lat": lat, "lon": lon}
            else:
                print(f"‚ùå [Places API] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{location}' | Status: {data['status']}")
        except Exception as e:
            print(f"‚ùå [Places API] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö '{location}': {e}")

        geocode_cache[location] = None
        return None

    
    def get_lat_lon_from_location_nominatim(location, geocode_cache):
        if location in geocode_cache:
            return geocode_cache[location]
        
        url = "https://nominatim.openstreetmap.org/search"
        params = {
            "q": location,
            "format": "json",
            "limit": 1
        }
        headers = {
            "User-Agent": "ExhibitionProject/1.0 (your_email@example.com)"
        }
        try:
            response = requests.get(url, params=params, headers=headers)
            data = response.json()
            if data:
                lat = float(data[0]["lat"])
                lon = float(data[0]["lon"])
                geocode_cache[location] = {"lat": lat, "lon": lon}
                time.sleep(1)
                return {"lat": lat, "lon": lon}
        except Exception as e:
            print(f"‚ùå [Nominatim] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Å‡∏±‡∏ö '{location}': {e}")
        
        geocode_cache[location] = None
        return None

    def get_lat_lon_with_fallback(location, geocode_cache):
        normalized_location = normalize_location(location)

        result = get_lat_lon_from_location_google(normalized_location, geocode_cache)
        if result:
            return result

        result = get_lat_lon_from_places_api(normalized_location, geocode_cache)
        if result:
            return result

        result = get_lat_lon_from_location_nominatim(normalized_location, geocode_cache)
        if result:
            return result

        return None



    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÉ‡∏ô DataFrame
    df["latitude"] = None
    df["longitude"] = None

    not_found_locations = []

    for idx, row in df.iterrows():
        loc = row.get("location")
        if isinstance(loc, str) and loc.strip():
            result = get_lat_lon_with_fallback(loc.strip(), geocode_cache)
            if result:
                df.at[idx, "latitude"] = result["lat"]
                df.at[idx, "longitude"] = result["lon"]
            else:
                not_found_locations.append(loc.strip())

    # üîê ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cache ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏£‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    with open(geocode_cache_path, "w", encoding="utf-8") as f:
        json.dump(geocode_cache, f, ensure_ascii=False, indent=2)

    # üßπ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    print(f"üßπ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    output_all_path = os.path.join(output_dir, f"merged_{mode}_{timestamp_str}.json")

    missing_count = df["latitude"].isnull().sum()

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ category_verified = False ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if 'category_verified' not in df.columns:
        df['category_verified'] = False
    else:
        df['category_verified'] = df['category_verified'].fillna(False)
    
    # 1. ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MongoDB ‚Üí datetime object ‡∏à‡∏£‡∏¥‡∏á
    df["start_date_obj"] = df["start_date"].apply(parse_thai_date_to_obj)
    df["end_date_obj"] = df["end_date"].apply(parse_thai_date_to_obj)

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡πÑ‡∏ß‡πâ export ‚Üí ‡πÅ‡∏õ‡∏•‡∏á datetime ‡πÄ‡∏õ‡πá‡∏ô string
    df_export = df.copy()
    df_export["start_date_obj"] = df_export["start_date_obj"].apply(lambda x: x.isoformat() if pd.notnull(x) else None)
    df_export["end_date_obj"] = df_export["end_date_obj"].apply(lambda x: x.isoformat() if pd.notnull(x) else None)

    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    with open(output_all_path, "w", encoding="utf-8") as f:
        json.dump(df_export.to_dict(orient="records"), f, ensure_ascii=False, indent=2)
        print(f"üìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á merge ‡∏ó‡∏µ‡πà: {output_all_path}")

# ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (‡∏£‡∏ß‡∏° full)
# python merge.py --mode=full

# ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡∏£‡∏ß‡∏° upcoming)
# python merge.py --mode=upcoming

