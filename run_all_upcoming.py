import os
import subprocess
import glob
import sys

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SPIDERS_DIR = os.path.join(BASE_DIR, 'scrapy_project', 'spiders')
MODE = 'upcoming'

def clear_old_data():
    print(f"üßπ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡πÉ‡∏ô raw_data/{MODE} ...")
    for root, dirs, files in os.walk(SPIDERS_DIR):
        raw_data_dir = os.path.join(root, 'raw_data', MODE)
        if os.path.exists(raw_data_dir):
            for file in glob.glob(os.path.join(raw_data_dir, '*.json')):
                try:
                    os.remove(file)
                    print(f"üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file}")
                except Exception as e:
                    print(f"‚ùå ‡∏•‡∏ö {file} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")

def run_spiders(suffix='_upcoming.py'):
    for root, dirs, files in os.walk(SPIDERS_DIR):
        for file in files:
            if file.endswith(suffix):
                spider_name = file.replace('.py', '')
                print(f'üï∑Ô∏è Running spider: {spider_name}')
                subprocess.run(
                    [sys.executable, '-m', 'scrapy', 'crawl', spider_name],
                    cwd=BASE_DIR  # ‚úÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ scrapy.cfg ‡∏ó‡∏µ‡πà BASE_DIR
                )

if __name__ == '__main__':
    clear_old_data()
    run_spiders('_upcoming.py')
