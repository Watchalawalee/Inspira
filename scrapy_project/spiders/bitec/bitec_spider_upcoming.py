import scrapy
import re
import json
import os
import datetime
from scrapy_project.category.predictor import predict_category
import dateparser
import html

class BitecSpider(scrapy.Spider):
    name = "bitec_spider_upcoming"
    allowed_domains = ["bitec.co.th"]
    handle_httpstatus_list = [500] 


    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏î‡∏¥‡∏°
        self.current_year = 2025
        self.current_month = 3
        self.current_page = 1
        self.empty_month_count = 0  # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
        self.past_event_count = 0  # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô past event

    def start_requests(self):
        url = self.build_url(self.current_year, self.current_month, self.current_page)
        yield scrapy.Request(url=url, callback=self.parse)

    def build_url(self, year, month, page):
        return f"https://www.bitec.co.th/wp-json/bitec/v1/events?page={page}&year={year}&month={month:02d}"

    def parse(self, response):
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ñ‡πâ‡∏≤‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏™‡πà‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ 500 (Internal Server Error)
        if response.status == 500:
            self.logger.warning(f"API error 500 at {response.url}, skipping to next month.")
            self.empty_month_count += 1
            yield from self.advance_to_next_month()
            return

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError:
            self.logger.warning(f"Invalid JSON response at {response.url}, skipping.")
            self.empty_month_count += 1
            yield from self.advance_to_next_month()
            return

        if data:
            self.empty_month_count = 0  # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏á
            for event in data:
                link = event.get("link")
                if link:
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° /th/ ‡∏´‡∏•‡∏±‡∏á domain
                    modified_link = link.replace("https://www.bitec.co.th/", "https://www.bitec.co.th/th/")
                    yield scrapy.Request(url=modified_link, callback=self.parse_event)

            # ‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ page ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            self.current_page += 1
            next_url = self.build_url(self.current_year, self.current_month, self.current_page)
            yield scrapy.Request(url=next_url, callback=self.parse)
        else:
            # ‡∏ñ‡πâ‡∏≤ page ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            self.logger.info(f"No data at {response.url}, moving to next month.")
            self.empty_month_count += 1
            yield from self.advance_to_next_month()

    def advance_to_next_month(self):
        self.current_page = 1
        self.current_month += 1

        if self.current_month > 12:
            self.current_month = 1
            self.current_year += 1

        if self.empty_month_count >= 10:
            self.logger.info("No data or server errors for 3 consecutive months. Stopping spider.")
            return  # ‡πÑ‡∏°‡πà yield ‡∏ï‡πà‡∏≠ = ‡∏´‡∏¢‡∏∏‡∏î spider

        next_url = self.build_url(self.current_year, self.current_month, self.current_page)
        yield scrapy.Request(url=next_url, callback=self.parse)


    def parse_event(self, response):
        # >>> ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ event <<< 
        self.logger.info(f"Processing event page: {response.url}")
        title = response.css("h2.entry-title a::text").get(default="null").strip()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å <header class="entry-header">
        date_text = response.css("header.entry-header p.date::text").get(default="null").strip()
        start_date, end_date = self.extract_dates_from_header(date_text)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å <header class="entry-header">
        location_text = response.css("header.entry-header p.venue::text").get(default="null").strip()
        location = self.clean_location(location_text)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå
        description = self.extract_description(response)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏•‡∏≤
        time_text = self.extract_text_by_keyword(response, ['‡πÄ‡∏ß‡∏•‡∏≤', 'Time'])
        event_slot_time = self.extract_time(time_text)

        # ‡∏´‡∏≤‡∏Å event_slot_time ‡πÄ‡∏õ‡πá‡∏ô "null" ‡∏´‡∏£‡∏∑‡∏≠ " - " ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô description ‡πÅ‡∏ó‡∏ô
        if event_slot_time == "null" or event_slot_time == " - ":
            event_slot_time = self.extract_time(description)

        # URL ‡∏Ç‡∏≠‡∏á‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå
        event_urls = self.extract_event_url(response)

        status = determine_status(start_date, end_date)
        if status not in ["upcoming", "ongoing"]:
            self.past_event_count += 1
            if self.past_event_count > 30:
                self.log("üõë ‡πÄ‡∏à‡∏≠ past event ‡πÄ‡∏Å‡∏¥‡∏ô 30 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‚Äî ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                raise scrapy.exceptions.CloseSpider("Too many past events")
            return

        # ‡∏ö‡∏±‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤
        ticket, ticket_price = self.extract_ticket_info(description)

        # ‡∏î‡∏∂‡∏á Cover Picture
        cover_picture = response.css("div.pic a img::attr(src)").get(default="null")

        reliability_score = 5
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        predicted_categories = predict_category(title, description)

        event_data = {
            "title": title if title else "null",
            "description": description,
            "categories": predicted_categories,
            "start_date": start_date,
            "end_date": end_date,
            "event_slot_time": event_slot_time if event_slot_time != "null" else " - ",
            "location": location,
            "url": event_urls,
            "ticket": ticket,
            "ticket_price": ticket_price,
            "cover_picture": cover_picture,
            "reliability_score": reliability_score,
            "timestamp": timestamp,
            "status": determine_status(start_date, end_date)
        }

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSON
        filename = re.sub(r"[\\/:*?\"<>|]", "_", title) + ".json"

        # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á spider ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ raw_data
        base_dir = os.path.dirname(os.path.abspath(__file__))
        raw_data_dir = os.path.join(base_dir, "raw_data", "upcoming")
        os.makedirs(raw_data_dir, exist_ok=True)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ

        filepath = os.path.join(raw_data_dir, filename)

        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(event_data, f, ensure_ascii=False, indent=4)

        yield event_data
    
    def extract_description(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏•‡∏ö \n, \t, ‡∏Ø‡∏•‡∏Ø """
        description_parts = []

        for p in response.css("div.entry-content p"):
            text = p.xpath("string(.)").get().strip()

            # ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°" ‡∏´‡∏£‡∏∑‡∏≠ "more information"
            if any(keyword in text.lower() for keyword in ["‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°", "more information"]):
                break

            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
            if not any(keyword in text.lower() for keyword in ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡πÄ‡∏ß‡∏•‡∏≤", "‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà", "date", "time", "venue"]):
                description_parts.append(text)

        if not description_parts:
            return "null"

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        combined_text = " ".join(description_parts)

        # ‡∏•‡∏ö \n, \t, \r ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ã‡πâ‡∏≥ ‡πÜ
        cleaned_text = re.sub(r"[\n\r\t]+", " ", combined_text)
        cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text).strip()

        return cleaned_text

    def extract_text_by_keyword(self, response, keywords):
        for p in response.css("div.entry-content p"):
            text = p.xpath("string(.)").get().strip()

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ keyword ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            if any(keyword in text for keyword in keywords):
                return text  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö keyword

        return "null"


    def extract_dates_from_header(self, text):
        month_map = {
            "January": "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°", "February": "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå", "March": "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°", "April": "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô",
            "May": "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°", "June": "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô", "July": "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°", "August": "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°",
            "September": "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô", "October": "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°", "November": "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô", "December": "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°"
        }

        pattern_range = r"(\d{1,2})\s*-\s*(\d{1,2})\s*(\w+)\s*(\d{4})"
        pattern_single = r"(\d{1,2})\s*(\w+)\s*(\d{4})"

        match_range = re.search(pattern_range, text)
        match_single = re.search(pattern_single, text)

        if match_range:
            start_day = int(match_range.group(1))
            end_day = int(match_range.group(2))
            month = match_range.group(3)
            year = int(match_range.group(4)) + 543  # ‡πÅ‡∏õ‡∏•‡∏á ‡∏Ñ.‡∏®. ‡πÄ‡∏õ‡πá‡∏ô ‡∏û.‡∏®.

            month_th = month_map.get(month, month)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            return f"{start_day} {month_th} {year}", f"{end_day} {month_th} {year}"

        elif match_single:
            day = int(match_single.group(1))
            month = match_single.group(2)
            year = int(match_single.group(3)) + 543

            month_th = month_map.get(month, month)
            return f"{day} {month_th} {year}", f"{day} {month_th} {year}"

        return "null", "null"


    def extract_time(self, text):
        # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô
        text = re.sub(r'\s+', ' ', text).strip()

        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á AM/PM ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
        def convert_to_24h(time_str):
            match = re.match(r"(\d{1,2})([:.]\d{2})?\s*(AM|PM)", time_str, re.IGNORECASE)
            if match:
                hour = int(match.group(1))
                minute = match.group(2) if match.group(2) else ":00"
                period = match.group(3).upper()

                if period == "PM" and hour != 12:
                    hour += 12
                if period == "AM" and hour == 12:
                    hour = 0

                return f"{hour:02d}{minute}".replace(".", ":")  # ‡πÅ‡∏õ‡∏•‡∏á 10.00 ‡πÄ‡∏õ‡πá‡∏ô 10:00
            return time_str  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô "09.00 ‚Äì 19.00 ‡∏ô.", "7:00 ‚Äì 17:00", "13.00 -16.00 ‡∏ô."
        pattern = r"(\d{1,2}[:.]\d{2})\s*[‚Äì\-]\s*(\d{1,2}[:.]\d{2})\s*(‡∏ô\.?)?"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "(‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á HH.MM - HH.MM ‡∏ô.)"
        show_pattern = r"\(‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á\s*(\d{1,2}[:.]\d{2})\s*[‚Äì\-]\s*(\d{1,2}[:.]\d{2})\s*‡∏ô?\.\)"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "‡∏£‡∏≠‡∏ö X: HH:MM ‡∏ô."
        round_pattern = r"‡∏£‡∏≠‡∏ö\s*\d+[:\s]*(\d{1,2}[:.]\d{2})\s*(‡∏ô\.?)?"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "‡πÄ‡∏ß‡∏•‡∏≤ HH:MM ‡∏ô."
        single_time_pattern = r"‡πÄ‡∏ß‡∏•‡∏≤[:\s]*(\d{1,2}[:.]\d{2})\s*(‡∏ô\.?)?"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "Time: 10am-6pm"
        am_pm_pattern = r"Time[:\s]*(\d{1,2}(?:[:.]\d{2})?\s*(?:AM|PM))\s*[-‚Äì]\s*(\d{1,2}(?:[:.]\d{2})?\s*(?:AM|PM))"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "Time 6 pm"
        single_am_pm_pattern = r"Time[:\s]*(\d{1,2}(?:[:.]\d{2})?)\s*(AM|PM)"

        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "Time: 10.00 AM. ‚Äì 09.00 PM."
        detailed_am_pm_pattern = r"Time[:\s]*(\d{1,2}[:.]\d{2})\s*(AM|PM)\s*[‚Äì\-]\s*(\d{1,2}[:.]\d{2})\s*(AM|PM)\."

        match = re.search(pattern, text)
        show_match = re.search(show_pattern, text)
        round_match = re.search(round_pattern, text)
        single_time_match = re.search(single_time_pattern, text)
        am_pm_match = re.search(am_pm_pattern, text)
        single_am_pm_match = re.search(single_am_pm_pattern, text)
        detailed_am_pm_match = re.search(detailed_am_pm_pattern, text)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥
        if match:
            start_time = match.group(1).replace(".", ":")
            end_time = match.group(2).replace(".", ":")
            return f"{start_time} - {end_time}"

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "(‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á HH.MM - HH.MM ‡∏ô.)"
        if show_match:
            start_time = show_match.group(1).replace(".", ":")
            end_time = show_match.group(2).replace(".", ":")
            return f"{start_time} - {end_time}"

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "‡∏£‡∏≠‡∏ö X: HH:MM ‡∏ô."
        if round_match:
            return round_match.group(1).replace(".", ":")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "‡πÄ‡∏ß‡∏•‡∏≤ HH:MM ‡∏ô."
        if single_time_match:
            return single_time_match.group(1).replace(".", ":")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "Time: 10am-6pm"
        if am_pm_match:
            start_time = convert_to_24h(am_pm_match.group(1))
            end_time = convert_to_24h(am_pm_match.group(2))
            return f"{start_time} - {end_time}"

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "Time 6 pm"
        if single_am_pm_match:
            return convert_to_24h(single_am_pm_match.group(1) + " " + single_am_pm_match.group(2))

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö "Time: 10.00 AM. ‚Äì 09.00 PM."
        if detailed_am_pm_match:
            start_time = convert_to_24h(detailed_am_pm_match.group(1) + " " + detailed_am_pm_match.group(2))
            end_time = convert_to_24h(detailed_am_pm_match.group(3) + " " + detailed_am_pm_match.group(4))
            return f"{start_time} - {end_time}"

        return "null"


    def clean_location(self, location):
        if location == "null":
            return "null"

        # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "BITEC" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ" ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
        location = re.sub(r"\b(BITEC|‡πÑ‡∏ö‡πÄ‡∏ó‡∏Ñ)\b", "", location, flags=re.IGNORECASE).strip()

        # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "Bitec " ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
        cleaned_location = f"Bitec {location}"

        # ‡∏•‡∏ö "," ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        cleaned_location = re.sub(r",\s*$", "", cleaned_location)

        return cleaned_location


    def extract_event_url(self, response):
        paragraphs = response.css("div.entry-content p")  # ‡∏î‡∏∂‡∏á <p> ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        found_inquiry = False  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠ "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°" ‡∏´‡∏£‡∏∑‡∏≠ "more information" ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

        for p in paragraphs:
            text = p.xpath("string(.)").get().strip().lower()

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°" ‡∏´‡∏£‡∏∑‡∏≠ "more information"
            if "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°" in text or "more information" in text:
                found_inquiry = True  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô p ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                continue

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠ "‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°" ‡∏´‡∏£‡∏∑‡∏≠ "more information" ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå
            if found_inquiry:
                links = p.css("a::attr(href)").getall()
                if links:
                    return links[0]  # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠

        return response.url  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ URL ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏ó‡∏ô

    def extract_ticket_info(self, description_text):
        ticket_keywords = ["‡∏ö‡∏±‡∏ï‡∏£", "ticket"]
        price_keywords = [
            "‡∏ö‡∏±‡∏ï‡∏£‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡∏±‡∏ï‡∏£", "‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°", "ticket price", "price",
            "entry fee", "admission", "entry ticket", "ticket fee"
        ]
        free_keywords = [
            "‡∏ü‡∏£‡∏µ", "free", "‡∏ö‡∏±‡∏ï‡∏£‡∏ü‡∏£‡∏µ", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏£‡∏µ", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
            "no charge", "free entry", "free admission"
        ]
        exclude_keywords = ["‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "service charge", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "surcharge"]
        skip_prices = {"67", "68", "23", "24", "25", "100"}

        clean_text = html.unescape(description_text)
        full_text = clean_text.lower()
        paragraphs = re.split(r"[.\n\r]", clean_text)

        ticket = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"
        raw_prices = []

        if any(free in full_text for free in free_keywords):
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°", None

        if any(keyword in full_text for keyword in ticket_keywords):
            ticket = "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"

        for p in paragraphs:
            p_clean = re.sub(r"<.*?>", "", p)
            lower_p = p_clean.lower()

            if any(ex_kw in lower_p for ex_kw in exclude_keywords):
                continue

            if any(price_kw in lower_p for price_kw in price_keywords) or "‡∏ö‡∏≤‡∏ó" in lower_p:
                matches = re.findall(r"\d{1,3}(?:,\d{3})+|\d{3,5}", lower_p)
                for m in matches:
                    try:
                        price = int(m.replace(",", ""))
                        if str(price) in skip_prices or re.match(r"^(25|20)\d{2}$", str(price)):
                            continue
                        if price >= 32:
                            raw_prices.append(price)
                    except:
                        continue

        filtered_prices = raw_prices
        if len(raw_prices) >= 2:
            max_price = max(raw_prices)
            threshold = max_price * 0.3
            filtered_prices = [p for p in raw_prices if p >= threshold]

        if ticket == "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°" and filtered_prices:
            ticket = "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"

        return ticket, sorted(filtered_prices) if filtered_prices else None
    
def convert_thai_year(text):
    match = re.search(r"(\d{1,2})\s+([^\s]+)\s+(\d{4})", text)
    if match:
        day, month, year = match.groups()
        year = int(year)
        if year > 2500:
            year -= 543
        return f"{day} {month} {year}"
    return text


def determine_status(start_date, end_date):
    today = datetime.date.today()
    start = dateparser.parse(convert_thai_year(start_date), languages=["th", "en"])
    end = dateparser.parse(convert_thai_year(end_date), languages=["th", "en"])

    if not start or not end:
        return "unknown"

    if today < start.date():
        return "upcoming"
    elif start.date() <= today <= end.date():
        return "ongoing"
    else:
        return "past"
