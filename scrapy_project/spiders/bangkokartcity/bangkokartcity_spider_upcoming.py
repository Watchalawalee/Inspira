import scrapy
import re
import json
import os
import datetime
from scrapy_project.category.predictor import predict_category
import dateparser
import html

class BangkokArtSpider(scrapy.Spider):
    name = "bangkokartcity_spider_upcoming"
    allowed_domains = ["bangkokartcity.org"]
    start_urls = ["https://www.bangkokartcity.org/th/discover"]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.past_event_count = 0  # ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô past event

    def parse(self, response):
        # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£
        exhibition_links = response.css("a.framer-vtg5wl.framer-1xv5ffg::attr(href)").getall()
        
        for link in exhibition_links:
            full_url = response.urljoin(link)
            yield scrapy.Request(full_url, callback=self.parse_exhibition)

    def parse_exhibition(self, response):
        title = response.css("h1.framer-text::text").get()
        description = " ".join(response.css("div.framer-129oip7 p.framer-text::text").getall()).strip()

        date_text = response.css("div.framer-u6f86y p.framer-text::text").get()
        start_date, end_date = self.extract_dates(date_text)

        event_slot_time_raw = response.css("div.framer-1xu6drh p.framer-text::text").get()
        event_slot_time = self.extract_time(event_slot_time_raw)

        location = response.css("div.framer-1mowxnk p.framer-text::text").get()

        status = determine_status(start_date, end_date)
        if status not in ["upcoming", "ongoing"]:
            self.past_event_count += 1
            if self.past_event_count > 30:
                self.log("üõë ‡πÄ‡∏à‡∏≠ past event ‡πÄ‡∏Å‡∏¥‡∏ô 30 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‚Äî ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                raise scrapy.exceptions.CloseSpider("Too many past events")
            return

        cover_picture = response.css("img[decoding='async']::attr(srcset)").get()
        if cover_picture:
            cover_picture = cover_picture.split(",")[-1].split(" ")[0]

        ticket, ticket_price = self.extract_ticket_info(description)

        if not title:
            self.logger.warning(f"Title not found for {response.url}")
            title = None

        reliability_score = 2
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        predicted_categories = predict_category(title, description)

        data = {
            "title": title.strip() if title else "null",
            "description": description if description else "No description",
            "start_date": start_date,
            "end_date": end_date,
            "categories": predicted_categories,
            "event_slot_time": event_slot_time if event_slot_time else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
            "location": location.strip() if location else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
            "url": response.url,
            "ticket": ticket,
            "ticket_price": ticket_price,
            "cover_picture": cover_picture if cover_picture else "No cover picture",
            "reliability_score": reliability_score,
            "timestamp": timestamp,
            "status": determine_status(start_date, end_date)

        }

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        if not data:
            self.logger.warning(f"No data extracted for {response.url}")
            return
        
         # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        filename = re.sub(r"[\\/:*?\"<>|]", "_", title) + ".json"

        # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á spider ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ raw_data
        base_dir = os.path.dirname(os.path.abspath(__file__))
        raw_data_dir = os.path.join(base_dir, "raw_data", "upcoming")
        os.makedirs(raw_data_dir, exist_ok=True)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ

        filepath = os.path.join(raw_data_dir, filename)

        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

        yield data
    
    def extract_dates(self, text):
        """ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏µ ‡∏û.‡∏®. ‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ.‡∏®. """
        months = {
            "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": 1, "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": 2, "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": 3, "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": 4, "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": 5, "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": 6,
            "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": 7, "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": 8, "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": 9, "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": 10, "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": 11, "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": 12
        }

        pattern = r"(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})\s*-\s*(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})"
        match = re.search(pattern, text)

        if match:
            start_day, start_month, start_year, end_day, end_month, end_year = match.groups()
            # ‡πÅ‡∏õ‡∏•‡∏á ‡∏û.‡∏®. ‚Üí ‡∏Ñ.‡∏®.
            start_year = int(start_year) - 543
            end_year = int(end_year) - 543

            start_date = f"{int(start_day)} {start_month} {start_year}"
            end_date = f"{int(end_day)} {end_month} {end_year}"
            return start_date, end_date

        return "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"

    
    def extract_time(self, text):
        """ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 10:00-17:00 """
        match = re.search(r"(\d{1,2}:\d{2}-\d{1,2}:\d{2})", text)
        return match.group(1) if match else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"

    def extract_ticket_info(self, description_text):
        ticket_keywords = ["‡∏ö‡∏±‡∏ï‡∏£", "ticket"]
        price_keywords = [
            "‡∏ö‡∏±‡∏ï‡∏£‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡∏±‡∏ï‡∏£", "‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°", "ticket price", "price",
            "entry fee", "admission", "entry ticket", "ticket fee"
        ]
        free_keywords = [
            "‡∏ü‡∏£‡∏µ", "free", "‡∏ö‡∏±‡∏ï‡∏£‡∏ü‡∏£‡∏µ", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏£‡∏µ", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
            "no charge", "free entry", "free admission"
        ]
        exclude_keywords = ["‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "service charge", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "surcharge"]
        skip_prices = {"67", "68", "23", "24", "25", "100"}

        clean_text = html.unescape(description_text)
        full_text = clean_text.lower()
        paragraphs = re.split(r"[.\n\r]", clean_text)

        ticket = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"
        raw_prices = []

        if any(free in full_text for free in free_keywords):
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°", None

        if any(keyword in full_text for keyword in ticket_keywords):
            ticket = "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"

        for p in paragraphs:
            p_clean = re.sub(r"<.*?>", "", p)
            lower_p = p_clean.lower()

            if any(ex_kw in lower_p for ex_kw in exclude_keywords):
                continue

            if any(price_kw in lower_p for price_kw in price_keywords) or "‡∏ö‡∏≤‡∏ó" in lower_p:
                matches = re.findall(r"\d{1,3}(?:,\d{3})+|\d{3,5}", lower_p)
                for m in matches:
                    try:
                        price = int(m.replace(",", ""))
                        if str(price) in skip_prices or re.match(r"^(25|20)\d{2}$", str(price)):
                            continue
                        if price >= 32:
                            raw_prices.append(price)
                    except:
                        continue

        filtered_prices = raw_prices
        if len(raw_prices) >= 2:
            max_price = max(raw_prices)
            threshold = max_price * 0.3
            filtered_prices = [p for p in raw_prices if p >= threshold]

        if ticket == "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°" and filtered_prices:
            ticket = "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"

        return ticket, sorted(filtered_prices) if filtered_prices else None
    
def determine_status(start_date, end_date):
    today = datetime.date.today()
    start = dateparser.parse(start_date, languages=["th", "en"])
    end = dateparser.parse(end_date, languages=["th", "en"])

    if not start or not end:
        return "unknown"

    if today < start.date():
        return "upcoming"
    elif start.date() <= today <= end.date():
        return "ongoing"
    else:
        return "past"


