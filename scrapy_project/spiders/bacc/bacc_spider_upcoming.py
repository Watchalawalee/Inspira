import scrapy
import re
import json
import os
from urllib.parse import urljoin
import datetime
from functools import partial
from scrapy_project.category.predictor import predict_category
import dateparser

class BACCSpider(scrapy.Spider):
    name = "bacc_spider_upcoming"
    allowed_domains = ["bacc.or.th"]
    start_urls = ["https://www.bacc.or.th/whats-on/page/1"]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.past_event_count = 0  # ‚úÖ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô past event

    def parse(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡∏≠‡∏á‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å """
        event_links = response.css("div.card.event a.card-link::attr(href)").getall()

        if not event_links:  
            self.logger.info("No more events found. Stopping the crawl.")
            return  

        for link in event_links:
            yield response.follow(link, callback=partial(self.parse_event, from_past=True))

        # ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡∏≠‡∏á pagination ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        pagination_links = response.css("ul.pagination li.page-item a.page-link::attr(href)").getall()

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå Pagination
        current_page = int(response.url.split("/")[-1])
        next_pages = [int(re.search(r'page/(\d+)', link).group(1)) for link in pagination_links if re.search(r'page/(\d+)', link)]
        
        # ‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        next_page = min([p for p in next_pages if p > current_page], default=None)

        if next_page:
            next_page_url = f"https://www.bacc.or.th/whats-on/page/{next_page}"
            self.logger.info(f"Following next page: {next_page_url}")
            yield response.follow(next_page_url, self.parse)
        else:
            # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Crawl "What's On" ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤ "Upcoming"
            self.logger.info("Finished crawling What's On, now moving to Upcoming events.")
            yield scrapy.Request(url="https://www.bacc.or.th/whats-on/upcoming", callback=self.parse_upcoming)

    def parse_upcoming(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏Ç‡∏≠‡∏á‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ Upcoming """
        event_links = response.css("div.card.event a.card-link::attr(href)").getall()
        
        if not event_links:
            self.logger.info("No upcoming events found.")
            return  

        for link in event_links:
            yield response.follow(link, self.parse_event)

        # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏î‡∏∂‡∏á upcoming ‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤ events
        self.logger.info("Finished crawling Upcoming, now moving to Past events.")
        yield scrapy.Request(url="https://www.bacc.or.th/events/page/1", callback=self.parse_past_events)


    def parse_event(self, response, from_past=False):
        """ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå """
        title = response.css("h3.mb-3::text").get(default="null").strip()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ç‡∏≠‡∏á‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        filename = self.sanitize_filename(title) + ".json"

        base_dir = os.path.dirname(os.path.abspath(__file__))
        raw_data_dir = os.path.join(base_dir, "raw_data")
        os.makedirs(raw_data_dir, exist_ok=True)
        filepath = os.path.join(raw_data_dir, filename)

        if os.path.exists(filepath):
            self.logger.info(f"Skipping already scraped event: {title}")
            return

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        date_text = response.xpath("string(//div[contains(@class, 'sb-box when')])").get()
        date_text = date_text.strip() if date_text else "null"

        start_date, end_date = self.extract_dates(date_text)

        # ‚úÖ ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å /events ‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏° < 2020-01-01 ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î
        if from_past and self.is_before_2020(start_date):
            self.logger.info(f"Reached event before 2020: {start_date} ‚Äî Stopping crawl.")
            raise scrapy.exceptions.CloseSpider("Reached event before 2020.")

        event_slot_time = "10.00-20.00"

        status = determine_status(start_date, end_date)
        if status not in ["upcoming", "ongoing"]:
            self.past_event_count += 1
            if self.past_event_count > 30:
                self.log("üõë ‡πÄ‡∏à‡∏≠ past event ‡πÄ‡∏Å‡∏¥‡∏ô 30 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‚Äî ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                raise scrapy.exceptions.CloseSpider("Too many past events")
            return

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà
        location = self.extract_location(response)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå
        description = self.extract_description(response)

        # URL ‡∏Ç‡∏≠‡∏á‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå
        event_urls = response.url

        # ‡∏ö‡∏±‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤
        ticket, ticket_price = self.extract_ticket_info(response)

        # ‡∏î‡∏∂‡∏á Cover Picture
        cover_picture = response.css("figure.wp-block-image a::attr(href)").get(default="null")

        reliability_score = 5
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        predicted_categories = predict_category(title, description)

        event_data = {
            "title": title if title else "null",
            "description": description if description else "null",
            "categories": predicted_categories,
            "start_date": start_date,
            "end_date": end_date,
            "event_slot_time": event_slot_time,
            "location": f"BACC {location}" if location != "null" else "null",
            "url": event_urls,
            "ticket": ticket,
            "ticket_price": ticket_price,
            "cover_picture": cover_picture,
            "reliability_score": reliability_score,
            "timestamp": timestamp,
            "status": determine_status(start_date, end_date)
        }

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        filename = re.sub(r"[\\/:*?\"<>|]", "_", title) + ".json"

        # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á spider ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ raw_data
        base_dir = os.path.dirname(os.path.abspath(__file__))
        raw_data_dir = os.path.join(base_dir, "raw_data", "upcoming")
        os.makedirs(raw_data_dir, exist_ok=True)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ

        filepath = os.path.join(raw_data_dir, filename)

        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(event_data, f, ensure_ascii=False, indent=4)

        yield event_data

    def extract_location(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà """
        location = response.xpath("string(//div[contains(@class, 'sb-box location')])").get()
        location = location.strip().replace("\n", " ") if location else "null"
        return location

    def extract_description(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å <p> ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô <a> """
        paragraphs = response.css("div.entry p:nth-of-type(n+2)")
        description = []

        for p in paragraphs:
            text = p.css("::text").getall()
            text = " ".join([t.strip() for t in text if t.strip()])

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô <a>
            link = p.css("a::attr(href)").get()
            if link:
                text += f" ({link})"

            if text:
                description.append(text)

        return " ".join(description) if description else "null"

    def extract_dates(self, text):
        """ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á ‡∏û.‡∏®. ‚Üí ‡∏Ñ.‡∏®. """

        def to_ad_year(year):
            year = int(year)
            return year - 543 if year > 2500 else year  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡∏û.‡∏®. ‡πÉ‡∏´‡πâ‡∏•‡∏ö 543

        # ‡∏Å‡∏£‡∏ì‡∏µ 1: ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏µ ‡πÄ‡∏ä‡πà‡∏ô "03 ‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏° 2567 - 31 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2568"
        full_range_pattern = r"(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})\s*-\s*(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})"
        match = re.search(full_range_pattern, text)

        if match:
            start_day, start_month, start_year, end_day, end_month, end_year = match.groups()
            return (
                f"{int(start_day)} {start_month} {to_ad_year(start_year)}",
                f"{int(end_day)} {end_month} {to_ad_year(end_year)}"
            )

        # ‡∏Å‡∏£‡∏ì‡∏µ 2: ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô "5 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° - 30 ‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏° 2568"
        range_pattern = r"(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*-\s*(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})"
        match = re.search(range_pattern, text)

        if match:
            start_day, start_month, end_day, end_month, year = match.groups()
            year = to_ad_year(year)
            return (
                f"{int(start_day)} {start_month} {year}",
                f"{int(end_day)} {end_month} {year}"
            )

        # ‡∏Å‡∏£‡∏ì‡∏µ 3: ‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏ä‡πà‡∏ô "02 ‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå 2563"
        single_pattern = r"(\d{1,2})\s*(‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s*(\d{4})"
        match = re.search(single_pattern, text)

        if match:
            day, month, year = match.groups()
            year = to_ad_year(year)
            return f"{int(day)} {month} {year}", f"{int(day)} {month} {year}"

        return "null", "null"


    def sanitize_filename(self, title):
        """ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á """
        invalid_chars = r'[\\/:*?"<>|]'
        return re.sub(invalid_chars, "_", title).strip()
    
    def extract_ticket_info(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ö‡∏±‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÇ‡∏î‡∏¢‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏° """
        paragraphs = response.css("div.entry p:nth-of-type(n+2)").getall()
        ticket_keywords = ["‡∏ö‡∏±‡∏ï‡∏£", "ticket"]
        price_keywords = [
            "‡∏ö‡∏±‡∏ï‡∏£‡∏£‡∏≤‡∏Ñ‡∏≤", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡∏±‡∏ï‡∏£", "‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°", "ticket price", "price",
            "entry fee", "admission", "entry ticket", "ticket fee"
        ]
        free_keywords = [
            "‡∏ü‡∏£‡∏µ", "free", "‡∏ö‡∏±‡∏ï‡∏£‡∏ü‡∏£‡∏µ", "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏£‡∏µ", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
            "no charge", "free entry", "free admission"
        ]
        exclude_keywords = ["‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "service charge", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "surcharge"]

        ticket = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"
        ticket_prices = []

        full_text = " ".join(paragraphs).lower()

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏£‡∏µ
        if any(free in full_text for free in free_keywords):
            return ticket, "null"

        if any(keyword in full_text for keyword in ticket_keywords + price_keywords):
            ticket = "‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"

            for p in paragraphs:
                p = re.sub(r"<.*?>", "", p)  # ‡∏•‡∏ö‡πÅ‡∏ó‡πá‡∏Å HTML ‡∏≠‡∏≠‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                lower_p = p.lower()

                # ‚ùå ‡∏Ç‡πâ‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô "‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°"
                if any(ex_kw in lower_p for ex_kw in exclude_keywords):
                    continue

                # ‚úÖ ‡∏´‡∏≤‡∏Å‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡∏±‡∏ï‡∏£ ‚Üí ‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                if any(price_kw in lower_p for price_kw in price_keywords):
                    prices = re.findall(r"\d{1,4}(?:,\d{3})*", lower_p)
                    ticket_prices.extend(prices)

        return ticket, ticket_prices if ticket_prices else "null"

    
    def parse_past_events(self, response):
        """ ‡∏î‡∏∂‡∏á‡∏ô‡∏¥‡∏ó‡∏£‡∏£‡∏®‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ß‡∏°‡∏≠‡∏µ‡πÄ‡∏ß‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤ """
        event_links = response.css("div.card.event a.card-link::attr(href)").getall()

        if not event_links:
            self.logger.info("No past events found.")
            return

        for link in event_links:
            yield response.follow(link, callback=lambda r, **kwargs: self.parse_event(r, from_past=True))

        # Pagination
        pagination_links = response.css("ul.pagination li.page-item a.page-link::attr(href)").getall()
        current_page = int(response.url.split("/")[-1])
        next_pages = [int(re.search(r'page/(\d+)', link).group(1)) for link in pagination_links if re.search(r'page/(\d+)', link)]
        next_page = min([p for p in next_pages if p > current_page], default=None)

        if next_page:
            next_page_url = f"https://www.bacc.or.th/events/page/{next_page}"
            self.logger.info(f"Following next past event page: {next_page_url}")
            yield response.follow(next_page_url, self.parse_past_events)

    def is_before_2020(self, start_date_str):
        """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ start_date < 1 ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° 2020 """
        try:
            months = {
                "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": 1, "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": 2, "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": 3, "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": 4,
                "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": 5, "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": 6, "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": 7, "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": 8,
                "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": 9, "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": 10, "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": 11, "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": 12
            }

            parts = start_date_str.split()
            if len(parts) != 3:
                return False

            day = int(parts[0])
            month = months.get(parts[1], 1)
            year = int(parts[2])
            date = datetime.date(year, month, day)

            return date < datetime.date(2020, 1, 1)

        except Exception as e:
            self.logger.warning(f"Failed to parse start_date '{start_date_str}': {e}")
            return False
        
def determine_status(start_date, end_date):
    today = datetime.date.today()
    start = dateparser.parse(start_date, languages=["th", "en"])
    end = dateparser.parse(end_date, languages=["th", "en"])

    if not start or not end:
        return "unknown"

    if today < start.date():
        return "upcoming"
    elif start.date() <= today <= end.date():
        return "ongoing"
    else:
        return "past"


